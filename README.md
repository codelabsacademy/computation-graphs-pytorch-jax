# Computation Graphs in PyTorch & JAX â€“ Companion Repo

This repository accompanies the **â€œDynamic vs. Static: Understanding Computation Graphs in PyTorch and JAXâ€** article on the Code Labs Academy blog.

It provides runnable, minimal code to exploreÂ exactly the ideas covered:

| Folder | Whatâ€™s inside |
|--------|---------------|
| `examples/` | Pureâ€‘PyTorch and pureâ€‘JAX scripts that build, visualise, and debug small networks. |
| `utils/`    | Helper functions for dumping/autopsy of computation graphs in both frameworks. |
| `benchmarks/` | A microâ€‘benchmark that reproduces the latency table in the article. |

## Quickâ€‘start


## Quick-Start

```bash
# 0. (optional but wise) upgrade pip so modern wheels resolve cleanly
python -m pip install --upgrade pip

# 1. Create & activate a virtual environment
python -m venv .venv && source .venv/bin/activate                     # Linux/macOS
# or: py -m venv .venv && .\.venv\Scripts\activate                    # Windows

# 2. Install dependencies (CPU-only default)
pip install -r requirements.txt

# 3. Run the examples
python examples/torch_example.py
python examples/jax_example.py
```

Apple-silicon note (M-series):
Make sure youâ€™re running a native arm64 Python (e.g. Homebrew, Miniforge).
An x86/Rosetta interpreter will crash JAX with an â€œAVX instructionsâ€ error because all x86 wheels are compiled with AVX.
If youâ€™re stuck on x86 Python, you can build jaxlib from sourceâ€”but using a native arm64 Python is far easier and faster.

> **Tip:** If youâ€™re on an AppleÂ Silicon or CPUâ€‘only machine, comment out the CUDA benchmark lines in `benchmark.py`.


**GPU optional:** The benchmark auto-detects CUDA.  
Force CPU with `DEVICE=cpu python benchmarks/benchmark.py`.

If you see giant â€œ___kmpc_* undefinedâ€ linker warnings: `torch.compile` produces OpenMP kernels; on Apple Mâ€‘series chips you have to provide the runtime manually.

```bash
# 1. install LibOMP and LLVM via Homebrew
$ brew install libomp llvm

# 2. give Clang access to the runtime
#    a) simplest: symlink into LLVM's prefix
$ sudo ln -s /opt/homebrew/opt/libomp/lib/libomp.dylib \
           /opt/homebrew/opt/llvm/lib/libomp.dylib

#    b) OR use a tiny wrapper that always links -lomp
$ mkdir -p ~/bin
$ cat <<'EOF' > ~/bin/clang++-omp
#!/usr/bin/env bash
exec /opt/homebrew/opt/llvm/bin/clang++ -fopenmp -lomp "$@"
EOF
$ chmod +x ~/bin/clang++-omp
$ export CXX=~/bin/clang++-omp

# 3. (optional) tell Torchâ€‘Inductor where headers and libs live
$ export CPATH=/opt/homebrew/include
$ export LIBRARY_PATH=/opt/homebrew/lib
```

## Requirements

See `requirements.txt`. Tested with:

* PythonÂ 3.10
* PyTorchÂ 2.3.0
* JAXÂ 0.4.27 / jaxlibÂ 0.4.27Â +Â CUDAÂ 12
* NVIDIAÂ RTXÂ 4090Â (benchmarks) â€“ but scripts also work on CPU.


## Running the Benchmark

```bash
# inside repo root, env activated
$ python benchmarks/benchmark.py

Batch    32 | **PyTorch:** 0.20â€¯ms | **torch.compile:** 3.67â€¯ms | **JAX:** 0.52â€¯ms
Batch  2048 | **PyTorch:** 6.33â€¯ms | **torch.compile:** 25.01â€¯ms | **JAX:** 9.88â€¯ms
```

## Learn More

* ğŸ“– Read the accompanying blog post â€”  
  **[Dynamic vs. Static: Understanding Computation Graphs in PyTorch and JAX](https://codelabsacademy.com/blog/en/computation-graphs-pytorch-jax)**

* ğŸ—‚ Explore the [examples/](examples/) folder to see each graph in action.

* ğŸ§ª New to automatic differentiation? The [FAQ](#faq) section links to papers and official docs.

* ğŸ§‘â€ğŸ“ Bootcamp: [Data Science and AI Bootcamp](https://codelabsacademy.com/en/courses/data-science-and-ai/) â€“ master ML essentials with live mentoring 

---

Made with â¤ï¸Â by CodeÂ LabsÂ Academy.  
Feel free to open issues or PRs!
